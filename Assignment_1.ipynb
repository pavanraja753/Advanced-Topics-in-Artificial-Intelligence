{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPz1f6MJ3oCIgg/7SKrmYLn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanraja753/Advanced-Topics-in-Artificial-Intelligence/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annotate code\n",
        "\n",
        "Consider a fully connected network constructed using the `__init__` method given below."
      ],
      "metadata": {
        "id": "-yKaF_xAHTB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Feed Forward computation:\n",
        "\n",
        " $z^{l} = w^{l}a^{l-1} + b^{l} \\tag{1}$\n",
        " $a^{l} = σ(z_{l}) \\tag{2}$ \n",
        "\n",
        "We assumed Sigmoid Non Linearity in our example\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qKDka1ClHmOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Aye3UwWwHDYr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)  # Number of layers in our neural network including the input layer\n",
        "        self.sizes = sizes            # Storing the sizes list in object variable to use it other methods in this class\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]  # We need to choose inital parameter numbers to compute the forward step and these weights will be in the gradient descent step\n",
        "                                      # From the Equation 1, we can see that b_l is a vector and it should be of size number of neurons in the layer l. Another point to note is that index 0 in sizes list \n",
        "                                      # represents the input layer and the weights and biases are defined from the hidden layer 1. That is the why we are iterating from index 1 from the sizes list \n",
        "        self.weights = [np.random.randn(y, x) # From the Equation 1, it is clear that Weights w should be of size num(previous_layer_neurons) * num(current_layer_neurons). and these weights are initialzed \n",
        "                                              # by sampling from the Standard Normal distribution (Mean =0, Varience =1)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):        \n",
        "        for b, w in zip(self.biases, self.weights): # Iterating over all the layers to compute the feedforward step in neural network in a recursive way. \n",
        "            a = sigmoid(np.dot(w, a)+b) # We are applying first the equation 1 : z_l = w_l * a_l-1 + b_l and the Equation 2: a_l = sigmoid(z_l) \n",
        "        return a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- Comment every code line in `backprop` with the analytical expression that line evaluates in computing $∇C$\n",
        "\n",
        "- Schematically apply `backprop` on a network constructed as `net = Network([784, 30, 10])`\n"
      ],
      "metadata": {
        "id": "PsmqfW-gI0x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back Propagation Equations\n",
        "\n",
        "Summary: The equations of backpropagation\n",
        "\n",
        "\\begin{align}\n",
        "\\delta^L = ∇_{a}C \\odot σ ^{'}(z^{L}) \\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\delta^l = (W^{l+1})^{T}\\delta^{l+1} \\odot σ ^{'}(z^{l}) \\tag{2} \n",
        "\\end{align}\n",
        "\n",
        "Using the error terms we compted the gradients with respect to the Weights and \n",
        "Biases in each layer\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{∂C}{∂b^l} = \\delta^{l} \\tag{3}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{∂C}{∂w^l} = \\delta^{l} (a^{l-1})^{T} \\tag{4}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "aONEKjAyJdpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop(self, x, y):\n",
        "    \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
        "    gradient for the cost function C_x.  \"nabla_b\" and\n",
        "    \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
        "    to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]  # Since gradient is computed for each parameter, we are \n",
        "                                                        #initializing the gradients of each bias parameter to be zero \n",
        "                                                        #with the same number of entries as biases   \n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights] # Since gradient is computed for each parameter, we are \n",
        "                                                        #initializing the gradients of each weight parameters to be zero \n",
        "                                                        #with the same number of entries as weight\n",
        "    # feedforward\n",
        "    activation = x                                      # Activation values \"a\" for the first layer is equal to input itself \n",
        "    activations = [x]                                   # list to store all the activations, layer by layer\n",
        "                                                        # We need to store all the intermediate activations to compute the backpropgation updates\n",
        "    zs = []                                             # list to store all the z vectors, layer by layer\n",
        "                                                        # all the intermediate Linear transformation values are also required for the backpropagation steps\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):         # Iterating over all the layers to compute the feedforward step in neural network in a recursive way.         \n",
        "        z = np.dot(w, activation)+b                     # computation of linear tranformation function z = Wx+b. where x is the activation map from the previous layers\n",
        "        zs.append(z)                                    # Storing the computations in a list to use it in the backpropagation step. In particualy we need these values to \n",
        "                                                        # compuate the derivative of sigmoid at these values. Equation 1 and 2 from the summary of backpropagation equations\n",
        "        activation = sigmoid(z)                         # Applying sigmoid non-lineariy function f(x) = 1/(1 + exp(-x))\n",
        "        activations.append(activation)                  # Storing the intermediate activation maps for the bavkpropagation steps. Equation 4 requires these values to compute \n",
        "                                                        # the partial derivate of Loss with respect to weights. \n",
        "    # backward pass\n",
        "    delta = (activations[-1] - y) * sigmoid_prime(zs[-1])  # This step computes the derivative of Loss with respect to \"z variables\" in the last layer of neural network. \n",
        "                                                           # Since we assumed to use squared loss function (a_i-y_i)^2 / 2, derivative of Loss with respect to output activation \n",
        "                                                           # values is (a_i - y_i) and the derivative of Loss with respect to \"z variables\" is product of \n",
        "                                                           #derivative of Loss with respect to \"output activation\" * derivative of sigmoid function computed at the z variable \n",
        "                                                           # Detailed computation of this step is provided in the below section of this code \n",
        "    nabla_b[-1] = delta                                    # Using Equation 3, we are computng the gradient with to \"b variables\" in the last layer of neural network. \n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose()) #Using Equation 4, we are computng the gradient with to \"w variables\" in the last layer of neural network.\n",
        "    for l in xrange(2, self.num_layers):               # Now, we are applying the recursive backpropagation step. Since we computed the delta values for the last layer, we use this last layer delta values and \n",
        "                                                       # Compute the delta for the previous layers. \n",
        "        z = zs[-l]                                     # From Equation 2, inorder to apply compte delta values recursively, we need to computed the derivative of sigmoid at the \"z variables\"\n",
        "        sp = sigmoid_prime(z)                          # 3rd term in Equation 2 RHS, requires the derivative of sigmoid at the \"z variables\"\n",
        "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp  # Computing the delta values recursively using equation 2 : delta_l = W^(l+1).T * delta_l+1 * derivative of sigma\n",
        "        nabla_b[-l] = delta                            # Using Equation 3 : C/ partial b = delta, we are computng the gradient with to \"b variables\" in the current layer of neural network. partial \n",
        "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) #Using Equation 4: C/ partial W = delta * a.T, we are computng the gradient with to \"w variables\" in the current layer of neural network.\n",
        "    return (nabla_b, nabla_w)"
      ],
      "metadata": {
        "id": "ScGeCZA8JB3G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Cost for a single training example is $C_x = \\frac{1}{2} ||y-a_L||^2 \\tag{1}$\n",
        "\n",
        "$\\frac{∂C}{∂{a_L}} = ||y-a_L||(-1) =||a_L-y|| \\tag{2}$\n",
        "\n",
        "-  From the notation $∇_{a}C = \\frac{∂C}{∂{a_L}} = ||a_L-y|| \\tag{3}$\n",
        "\n",
        "The above relation is directly used in the Equation 1 of summary of backpropgation steps.\n",
        "- This formula will change depending on the choice of loss function"
      ],
      "metadata": {
        "id": "nPR_h_xikZVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)  # Number of layers in our neural network including the input layer\n",
        "        self.sizes = sizes            # Storing the sizes list in object variable to use it other methods in this class\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]  # We need to choose inital parameter numbers to compute the forward step and these weights will be in the gradient descent step\n",
        "                                      # From the Equation 1, we can see that b_l is a vector and it should be of size number of neurons in the layer l. Another point to note is that index 0 in sizes list \n",
        "                                      # represents the input layer and the weights and biases are defined from the hidden layer 1. That is the why we are iterating from index 1 from the sizes list \n",
        "        self.weights = [np.random.randn(y, x) # From the Equation 1, it is clear that Weights w should be of size num(previous_layer_neurons) * num(current_layer_neurons). and these weights are initialzed \n",
        "                                              # by sampling from the Standard Normal distribution (Mean =0, Varience =1)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):        \n",
        "        for b, w in zip(self.biases, self.weights): # Iterating over all the layers to compute the feedforward step in neural network in a recursive way. \n",
        "            a = sigmoid(np.dot(w, a)+b) # We are applying first the equation 1 : z_l = w_l * a_l-1 + b_l and the Equation 2: a_l = sigmoid(z_l) \n",
        "        return a\n",
        "\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
        "        gradient for the cost function C_x.  \"nabla_b\" and\n",
        "        \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
        "        to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]  # Since gradient is computed for each parameter, we are \n",
        "                                                            #initializing the gradients of each bias parameter to be zero \n",
        "                                                            #with the same number of entries as biases   \n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights] # Since gradient is computed for each parameter, we are \n",
        "                                                            #initializing the gradients of each weight parameters to be zero \n",
        "                                                            #with the same number of entries as weight\n",
        "        # feedforward\n",
        "        activation = x                                      # Activation values \"a\" for the first layer is equal to input itself \n",
        "        activations = [x]                                   # list to store all the activations, layer by layer\n",
        "                                                            # We need to store all the intermediate activations to compute the backpropgation updates\n",
        "        zs = []                                             # list to store all the z vectors, layer by layer\n",
        "                                                            # all the intermediate Linear transformation values are also required for the backpropagation steps\n",
        "\n",
        "        for b, w in zip(self.biases, self.weights):         # Iterating over all the layers to compute the feedforward step in neural network in a recursive way.         \n",
        "            z = np.dot(w, activation)+b                     # computation of linear tranformation function z = Wx+b. where x is the activation map from the previous layers\n",
        "            zs.append(z)                                    # Storing the computations in a list to use it in the backpropagation step. In particualy we need these values to \n",
        "                                                            # compuate the derivative of sigmoid at these values. Equation 1 and 2 from the summary of backpropagation equations\n",
        "            activation = sigmoid(z)                         # Applying sigmoid non-lineariy function f(x) = 1/(1 + exp(-x))\n",
        "            activations.append(activation)                  # Storing the intermediate activation maps for the bavkpropagation steps. Equation 4 requires these values to compute \n",
        "                                                            # the partial derivate of Loss with respect to weights. \n",
        "        # backward pass\n",
        "        delta = (activations[-1] - y) * sigmoid_prime(zs[-1])  # This step computes the derivative of Loss with respect to \"z variables\" in the last layer of neural network. \n",
        "                                                            # Since we assumed to use squared loss function (a_i-y_i)^2 / 2, derivative of Loss with respect to output activation \n",
        "                                                            # values is (a_i - y_i) and the derivative of Loss with respect to \"z variables\" is product of \n",
        "                                                            #derivative of Loss with respect to \"output activation\" * derivative of sigmoid function computed at the z variable \n",
        "                                                            # Detailed computation of this step is provided in the below section of this code \n",
        "        nabla_b[-1] = delta                                    # Using Equation 3, we are computng the gradient with to \"b variables\" in the last layer of neural network. \n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) #Using Equation 4, we are computng the gradient with to \"w variables\" in the last layer of neural network.\n",
        "        for l in range(2, self.num_layers):               # Now, we are applying the recursive backpropagation step. Since we computed the delta values for the last layer, we use this last layer delta values and \n",
        "                                                        # Compute the delta for the previous layers. \n",
        "            z = zs[-l]                                     # From Equation 2, inorder to apply compte delta values recursively, we need to computed the derivative of sigmoid at the \"z variables\"\n",
        "            sp = sigmoid_prime(z)                          # 3rd term in Equation 2 RHS, requires the derivative of sigmoid at the \"z variables\"\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp  # Computing the delta values recursively using equation 2 : delta_l = W^(l+1).T * delta_l+1 * derivative of sigma\n",
        "            nabla_b[-l] = delta                            # Using Equation 3 : C/ partial b = delta, we are computng the gradient with to \"b variables\" in the current layer of neural network. partial \n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) #Using Equation 4: C/ partial W = delta * a.T, we are computng the gradient with to \"w variables\" in the current layer of neural network.\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta):\n",
        "        \"\"\"Train the neural network using mini-batch stochastic\n",
        "        gradient descent.  The ``training_data`` is a list of tuples\n",
        "        ``(x, y)`` representing the training inputs and the desired\n",
        "        outputs.  The other non-optional parameters are\n",
        "        self-explanatory.  \"\"\"\n",
        "        n = len(training_data)\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in xrange(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Update the network's weights and biases by applying\n",
        "        gradient descent using backpropagation to a single mini batch.\n",
        "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
        "        is the learning rate.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "W-511l0GtaU4"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Network([784,30,10])"
      ],
      "metadata": {
        "id": "SksBZun82R4f"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.feedforward(np.random.randn(784,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcaUSIe72UwT",
        "outputId": "3a3a96d4-e697-441b-9398-9ea798734dcd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.82417706],\n",
              "       [0.00781638],\n",
              "       [0.98149145],\n",
              "       [0.98866133],\n",
              "       [0.55647665],\n",
              "       [0.91949438],\n",
              "       [0.00309173],\n",
              "       [0.43049102],\n",
              "       [0.0557094 ],\n",
              "       [0.98392345]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_b,nabla_w = net.backprop(np.random.randn(784,1),np.array([1,0,0,0,0,0,0,0,0,0]).reshape(10,1))"
      ],
      "metadata": {
        "id": "wDo9soK52ZLE"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_b[0].shape, nabla_b[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2SSlT343A51",
        "outputId": "d5069513-9978-44dd-da49-be68dae7a9f7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30, 1), (10, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_w[0].shape, nabla_w[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClCUhsYv3Tvl",
        "outputId": "1acf64fb-ae76-407f-9f20-86033a467302"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30, 784), (10, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = (np.random.randn(10000,784), np.random.randn(10000,10))"
      ],
      "metadata": {
        "id": "L5XuTdnb3ZEG"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}