{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1",
      "provenance": [],
      "authorship_tag": "ABX9TyNnEvmnKxZFpGFp+qnjxZ2/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanraja753/Advanced-Topics-in-Artificial-Intelligence/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annotate code\n",
        "\n",
        "Consider a fully connected network constructed using the `__init__` method given below."
      ],
      "metadata": {
        "id": "-yKaF_xAHTB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Feed Forward computation:\n",
        "\n",
        "- $z^{l} = w^{l}a^{l-1} + b^{l}$\n",
        "- $a^{l} = σ(z_{l})$ \n",
        "\n",
        "We assumed Sigmoid Non Linearity in our example\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qKDka1ClHmOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aye3UwWwHDYr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) \n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- Comment every code line in `backprop` with the analytical expression that line evaluates in computing $∇C$\n",
        "\n",
        "- Schematically apply `backprop` on a network constructed as `net = Network([784, 30, 10])`\n"
      ],
      "metadata": {
        "id": "PsmqfW-gI0x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back Propagation Equations\n",
        "\n",
        "Summary: The equations of backpropagation\n",
        "\n",
        "\\begin{align}\n",
        "\\delta^L = ∇_{a}C \\odot σ ^{'}(z^{L}) \\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\delta^l = (W^{l+1})^{T}\\delta^{l+1} \\odot σ ^{'}(z^{l}) \\tag{2} \n",
        "\\end{align}\n",
        "\n",
        "Using the error terms we compted the gradients with respect to the Weights and \n",
        "Biases in each layer\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{∂C}{∂b^l} = \\delta^{l} \\tag{3}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{∂C}{∂w^l} = \\delta^{l} (a^{l-1})^{T} \\tag{4}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "aONEKjAyJdpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop(self, x, y):\n",
        "    \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
        "    gradient for the cost function C_x.  \"nabla_b\" and\n",
        "    \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
        "    to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]  # Since gradient is computed for each parameter, we are \n",
        "                                                        #initializing the gradients of each bias parameter to be zero \n",
        "                                                        #with the same number of entries as biases   \n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights] # Since gradient is computed for each parameter, we are \n",
        "                                                        #initializing the gradients of each weight parameters to be zero \n",
        "                                                        #with the same number of entries as weight\n",
        "    # feedforward\n",
        "    activation = x                                      # Activation values \"a\" for the first layer is equal to input itself \n",
        "    activations = [x]                                   # list to store all the activations, layer by layer\n",
        "                                                        # We need to store all the intermediate activations to compute the backpropgation updates\n",
        "    zs = []                                             # list to store all the z vectors, layer by layer\n",
        "                                                        # all the intermediate Linear transformation values are also required for the backpropagation steps\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):         # Iterating over all the layers to compute the feedforward step in neural network in a recursive way.         \n",
        "        z = np.dot(w, activation)+b                     # computation of linear tranformation function z = Wx+b. where x is the activation map from the previous layers\n",
        "        zs.append(z)                                    # Storing the computations in a list to use it in the backpropagation step. In particualy we need these values to \n",
        "                                                        # compuate the derivative of sigmoid at these values. Equation 1 and 2 from the summary of backpropagation equations\n",
        "        activation = sigmoid(z)                         # Applying sigmoid non-lineariy function f(x) = 1/(1 + exp(-x))\n",
        "        activations.append(activation)                  # Storing the intermediate activation maps for the bavkpropagation steps. Equation 4 requires these values to compute \n",
        "                                                        # the partial derivate of Loss with respect to weights. \n",
        "    # backward pass\n",
        "    delta = (activations[-1] - y) * sigmoid_prime(zs[-1])  # This step computes the derivative of Loss with respect to \"z variables\" in the last layer of neural network. \n",
        "                                                           # Since we assumed to use squared loss function (a_i-y_i)^2 / 2, derivative of Loss with respect to output activation \n",
        "                                                           # values is (a_i - y_i) and the derivative of Loss with respect to \"z variables\" is product of \n",
        "                                                           #derivative of Loss with respect to \"output activation\" * derivative of sigmoid function computed at the z variable \n",
        "                                                           # Detailed computation of this step is provided in the below section of this code \n",
        "    nabla_b[-1] = delta                                    # Using Equation 3, we are computng the gradient with to \"b variables\" in the last layer of neural network. \n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose()) #Using Equation 4, we are computng the gradient with to \"w variables\" in the last layer of neural network.\n",
        "    for l in xrange(2, self.num_layers):               # Now, we are applying the recursive backpropagation step. Since we computed the delta values for the last layer, we use this last layer delta values and \n",
        "                                                       # Compute the delta for the previous layers. \n",
        "        z = zs[-l]                                     # From Equation 2, inorder to apply compte delta values recursively, we need to computed the derivative of sigmoid at the \"z variables\"\n",
        "        sp = sigmoid_prime(z)                          # 3rd term in Equation 2 RHS, requires the derivative of sigmoid at the \"z variables\"\n",
        "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp  # Computing the delta values recursively using equation 2\n",
        "        nabla_b[-l] = delta                            # Using Equation 3, we are computng the gradient with to \"b variables\" in the current layer of neural network.\n",
        "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) #Using Equation 4, we are computng the gradient with to \"w variables\" in the current layer of neural network.\n",
        "    return (nabla_b, nabla_w)"
      ],
      "metadata": {
        "id": "ScGeCZA8JB3G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Cost for a single training example is $C_x = \\frac{1}{2} ||y-a_L||^2$\n",
        "\n",
        "- $\\frac{∂C}{∂{a_L}} = ||y-a_L||(-1) =||a_L-y||$\n",
        "\n",
        "-  From the notation $∇_{a}C = \\frac{∂C}{∂{a_L}} = ||a_L-y|| $\n",
        "\n",
        "The above relation is directly used in the Equation 1 of summary of backpropgation steps.\n",
        "- This formula will change depending on the choice of loss function"
      ],
      "metadata": {
        "id": "nPR_h_xikZVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Tri6yoPVeX6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}