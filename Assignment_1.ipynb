{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1",
      "provenance": [],
      "collapsed_sections": [
        "aONEKjAyJdpx"
      ],
      "authorship_tag": "ABX9TyNIxW4iF56lM7hCbO3atA2L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanraja753/Advanced-Topics-in-Artificial-Intelligence/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annotate code\n",
        "\n",
        "Consider a fully connected network constructed using the `__init__` method given below."
      ],
      "metadata": {
        "id": "-yKaF_xAHTB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Feed Forward computation:\n",
        "\n",
        " $z^{l} = w^{l}a^{l-1} + b^{l} \\tag{1}$\n",
        " $a^{l} = σ(z_{l}) \\tag{2}$ \n",
        "\n",
        "We assumed Sigmoid Non Linearity in our example\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qKDka1ClHmOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aye3UwWwHDYr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)  # Number of layers in our neural network including the input layer\n",
        "        self.sizes = sizes            # Storing the sizes list in object variable to use it other methods in \n",
        "                                      # this class\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]  # We need to choose inital parameter numbers \n",
        "                                      # to compute the forward step and these weights will be in the gradient descent step\n",
        "                                      # From the Equation 1, we can see that b_l is a vector and it should be of size number of neurons in the layer l. Another point to note is that index 0 in sizes list \n",
        "                                      # represents the input layer and the weights and biases are defined from the hidden layer 1. That is the why we are iterating from index 1 from the sizes list \n",
        "        self.weights = [np.random.randn(y, x) # From the Equation 1, it is clear that Weights w should be of size num(previous_layer_neurons) * num(current_layer_neurons). and these weights are initialzed \n",
        "                                              # by sampling from the Standard Normal distribution (Mean =0, Varience =1)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):        \n",
        "        for b, w in zip(self.biases, self.weights): # Iterating over all the layers to compute the feedforward step in neural network in a recursive way. \n",
        "            a = sigmoid(np.dot(w, a)+b) # We are applying first the equation 1 : z_l = w_l * a_l-1 + b_l and the Equation 2: a_l = sigmoid(z_l) \n",
        "        return a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- Comment every code line in `backprop` with the analytical expression that line evaluates in computing $∇C$\n",
        "\n",
        "- Schematically apply `backprop` on a network constructed as `net = Network([784, 30, 10])`\n"
      ],
      "metadata": {
        "id": "PsmqfW-gI0x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back Propagation Equations\n",
        "\n",
        "Summary: The equations of backpropagation\n",
        "\n",
        "\\begin{align}\n",
        "\\delta^L = ∇_{a}C \\odot σ ^{'}(z^{L}) \\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\delta^l = (W^{l+1})^{T}\\delta^{l+1} \\odot σ ^{'}(z^{l}) \\tag{2} \n",
        "\\end{align}\n",
        "\n",
        "Using $\\delta_{L}$ (Last Layer) and $\\delta_{l}$ (For the remaining layer) we compted the gradients with respect to the Weights and \n",
        "Biases in each layer\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{∂C}{∂b^l} = \\delta^{l} \\tag{3}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{∂C}{∂w^l} = \\delta^{l} (a^{l-1})^{T} \\tag{4}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "**Copmutation of $∇_aC$ is shown below**\n",
        "\n",
        "- Cost for a single training example is $C_x = \\frac{1}{2} ||y-a_L||^2 \\tag{5}$\n",
        "\n",
        "$\\frac{∂C}{∂{a_L}} = (y-a_L)(-1) =(a_L-y) \\tag{6}$\n",
        "\n",
        "-  From the notation $∇_{a}C = \\frac{∂C}{∂{a_L}} = (a_L-y) \\tag{7}$\n",
        "\n",
        "The above relation 7 is directly used in the Equation 1 of summary of backpropgation steps.\n",
        "- *Note: This formula will change depending on the choice of loss function*"
      ],
      "metadata": {
        "id": "aONEKjAyJdpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop(self, x, y):\n",
        "    \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
        "    gradient for the cost function C_x.  \"nabla_b\" and\n",
        "    \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
        "    to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]  # Since gradient is computed for each parameter, we are \n",
        "                                                        #initializing the gradients of each bias parameter to be zero \n",
        "                                                        #with the same number of entries as biases parameters  \n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights] # Since gradient is computed for each parameter, we are \n",
        "                                                        #initializing the gradients of each weight parameters to be zero \n",
        "                                                        #with the same number of entries as weight parameters\n",
        "    # feedforward\n",
        "    activation = x                                      # Activation values \"a\" for the first layer is equal to input \"x\" itself \n",
        "    activations = [x]                                   # list to store all the activations, layer by layer\n",
        "                                                        # We need to store all the intermediate activations to compute the backpropgation updates\n",
        "    zs = []                                             # list to store all the z vectors, layer by layer (zs variable represent before the sigmoid function is applied)\n",
        "                                                        # all the intermediate Linear transformation values are also required for the backpropagation steps\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):         # Iterating over all the layers to compute the feedforward step in neural network in a recursive way.         \n",
        "        z = np.dot(w, activation)+b                     # computation of linear tranformation function z = Wx+b. where x is the activation map from the previous layers. np.dot is matrix multiplcation\n",
        "        zs.append(z)                                    # Storing the computations in a list to use it in the backpropagation step. In particualy we need these values to \n",
        "                                                        # compuate the derivative of sigmoid at these values. Equation 1 and 2 from the summary of backpropagation equations\n",
        "        activation = sigmoid(z)                         # Applying sigmoid non-lineariy function f(x) = 1/(1 + np.exp(-x)) element wise. (Numpy applies element wise to vectore quantity)\n",
        "        activations.append(activation)                  # Storing the intermediate activation maps for the backpropagation steps. Equation 4 requires these values to compute \n",
        "                                                        # the partial derivate of Loss with respect to weights. \n",
        "    # backward pass\n",
        "    delta = (activations[-1] - y) * sigmoid_prime(zs[-1])  # This step computes the derivative of Loss with respect to \"z variables\" in the last layer of neural network. \n",
        "                                                           # Since we assumed to use squared loss function (a_i-y_i)^2 / 2, derivative of Loss with respect to output activation \n",
        "                                                           # values is (a_i - y_i) and the derivative of Loss with respect to \"z variables\" is product of \n",
        "                                                           #derivative of Loss with respect to \"output activation\" * derivative of sigmoid function computed at the z variable \n",
        "                                                           # Detailed computation of this step is provided in the above section of this code (Equations 5, 6, 7) \n",
        "    nabla_b[-1] = delta                                    # Using Equation 3, we are computng the gradient with to \"b variables\" in the last layer of neural network. \n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose()) #Using Equation 4, we are computng the gradient with to \"w variables\" in the last layer of neural network.\n",
        "    for l in xrange(2, self.num_layers):               # Now, we are applying the recursive backpropagation step. Since we already computed the delta values for the last layer, we use this last layer delta values and \n",
        "                                                       # Compute the delta for the previous layers. \n",
        "        z = zs[-l]                                     # From Equation 2, inorder to apply compte delta values recursively, we need to computed the derivative of sigmoid at the \"z variables\"\n",
        "        sp = sigmoid_prime(z)                          # 3rd term in Equation 2 RHS, requires the derivative of sigmoid at the \"z variables\"\n",
        "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp  # Computing the delta values recursively using equation 2 : delta_l = W^(l+1).T * delta_l+1 * derivative of sigma\n",
        "        nabla_b[-l] = delta                            # Using Equation 3 : C/ partial b = delta, we are computng the gradient with to \"b variables\" in the current layer of neural network. partial \n",
        "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) #Using Equation 4: partial C/ partial W = delta * a.T, we are computng the gradient with to \"w variables\" in the current layer of neural network.\n",
        "    return (nabla_b, nabla_w)"
      ],
      "metadata": {
        "id": "ScGeCZA8JB3G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II Using matrix notation\n",
        "\n",
        "### Forward Step\n",
        "\n",
        "\n",
        "\n",
        "Summary of Feed Forward computation:\n",
        "\n",
        " $z^{l} = w^{l}a^{l-1} + b^{l} \\tag{1}$\n",
        " $a^{l} = σ(z^{l}) \\tag{2}$ <br><br>\n",
        "\n",
        "\n",
        "Let us compute the z values for the first layer: $z^{1} = w^{1}a^{0} + b^{1}$\n",
        "  and $a^{1} = σ(z_{1})$ \n",
        "\n",
        " Since $a^0 $ is equal to input $x$ Equation 1 becomes $z^{1} = w^{1}x + b^{1}$ <br><br>\n",
        "\n",
        "\n",
        " Lets process the input as a mini-batch of size m. Take all the inputs for all the datapoints from $1,2,3....m$ and stack the vectors along the column. Now the data matrix becomes\n",
        "\n",
        "\\begin{equation}\n",
        "X=\n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    x_1   & x_2 & . & . &. &x_m   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}\n",
        "\\end{equation} \n",
        " \n",
        "where $x_i$ representing input vector of $i^{th}$ instance <br><br>\n",
        "\n",
        "From the Equation $z^{1} = w^{1}x + b^{1}$ replacing $x$ with $X$ results in the following expression\n",
        "\n",
        "$z^{1} = w^{1}X + b^{1} \\tag{5}$\n",
        "\n",
        "Expanding the above expression and use the matrix matrix product with one column at at time\n",
        "\n",
        "\\begin{equation}\n",
        "z^1= w^1\n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    x_1   & x_2 & . & . &. &x_m   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}\n",
        "+ b^1 \n",
        "\\end{equation} \n",
        "\n",
        "\\begin{equation}\n",
        "z^1= \n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    w^1x_1   & w^1x_2 & . & . &. &w^1x_m   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}\n",
        "+ b^1 \n",
        "\\tag{7}\n",
        "\\end{equation} \n",
        "\n",
        "Observing the equation 7, Each column in the resulting matrix correpsonds to genuine computation of $w^1x_i$ and repeating the columns of vector $b^1$, m times this will exactly corresponds to forward computation of our deep neural network model <br><br>\n",
        "\n",
        "**In Summary: By Stacking the input data along the column dimensing and repeating the columns of $b^1$ vector total m times (mini-batch size), we can directly compute $z^{1} = w^{1}X + b^{1}$ and each column in $z^1$ matrix corresponds to one data instance. There is no change to be done during the forward step in the code. Numpy broadcasting operation also helps in repeating the bias vectors to match the mini-batch dimension**\n",
        "\n",
        "***Note:In Python We dont have to repeat the columns of $b^1$, since numpy has broadcasting operation***\n",
        "\n",
        "Similary, since we are applying sigmoid function element-wise to the \"z\" variable. Each column in $a^1$ matrix corresponds to one data instance <br><br><br>\n",
        "\n",
        "#### **Conclusion: When we modify the input x with X containing all the mini-batch instances stacked along the column dimension, all the intermediate $z^l$ variables and $a^l$ variables and the last layer $z^L$ , $a^L$ are matrices and each $i^{th}$ column in these matrices corresponds to $i^{th}$ data instance**  \n",
        "\n",
        "\n",
        "## Backpropagation equations\n",
        "\n",
        "$∇_{a}C = \\frac{∂C}{∂{a_L}} = (a_L-y) \\tag{1}$\n",
        "\n",
        "\\begin{align}\n",
        "\\delta^L = ∇_{a}C \\odot σ ^{'}(z^{L}) \\tag{2}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\delta^l = (W^{l+1})^{T}\\delta^{l+1} \\odot σ ^{'}(z^{l}) \\tag{3} \n",
        "\\end{align}\n",
        "\n",
        "Using the error terms we compted the gradients with respect to the Weights and \n",
        "Biases in each layer\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{∂C}{∂b^l} = \\delta^{l} \\tag{4}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{∂C}{∂w^l} = \\delta^{l} (a^{l-1})^{T} \\tag{5}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Analyzing Equation 1: $∇_{a}C = \\frac{∂C}{∂{a_L}} = (a_L-y)$**\n",
        "\n",
        "- From the above argument when we represent input data with $\n",
        "X=\n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    x_1   & x_2 & . & . &. &x_m   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}\n",
        "$ then the activation matrix at the last layer becomes $a_L=\n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    a_{L1}   & a_{L2} & . & . &. &a_{Lm}   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}$ where $a_{Li}$ represents activation for the $i^{th}$ data sample.\n",
        "\n",
        "- Similarly if we encode the output as input $Y=\n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    y_1   & y_2 & . & . &. &y_m   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix} $ then $i^{th}$ column in $a_L-Y$ matrix corresponds to $i^{th}$ data instance in mini-batch\n",
        "\n",
        "\n",
        "<br><br>\n",
        "**Analyzing Equation 2: $ \\delta^L = ∇_{a}C \\odot σ ^{'}(z^{L})$**\n",
        "\n",
        "\n",
        "- Since this equation $ \\delta^L = ∇_{a}C \\odot σ ^{'}(z^{L})$ involves element wise multiplication. Each column $(i^{th})$in the matrix $\\delta^L$ corresponds to $i^{th}$ data instance in mini-batch\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Analyzing Equation 3: $ \\delta^l = (W^{l+1})^{T}\\delta^{l+1} \\odot σ ^{'}(z^{l}) $**\n",
        "\n",
        "- When we process the mini-batch of data then each columns in the matrix $\\delta^{L}$ corresponds to $i^{th}$ example (from the above argument). Expanding $\\delta^{l+1}$ in terms of each columns $\\delta^{l+1}=\n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    \\delta^{l+1}_{1}   & \\delta^{l+1}_{2} & . & . &. &\\delta^{l+1}_{m}   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}$ and $σ ^{'}(z^l) = \\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    σ ^{'}(z_{1}^l)  & σ ^{'}(z_{2}^l) & . & . &. &σ ^{'}(z_{m}^l)   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n",
        "\n",
        "- Then Equation 3 becomes $\\delta^{l}=(W^{l+1})^{T} \\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    \\delta^{l+1}_{1}   & \\delta^{l+1}_{2} & . & . &. &\\delta^{l+1}_{m}   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix} ⊙ \\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    σ ^{'}(z_{1}^l)  & σ ^{'}(z_{2}^l) & . & . &. &σ ^{'}(z_{m}^l)   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix} \\tag{8}\n",
        "$\n",
        "\n",
        "- Using the Property $A\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    col_1   & col_2 & . & . &. &col_m   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    Acol_1   & Acol_2 & . & . &. &Acol_m   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}$\n",
        "\n",
        "- Equation 3 Simplifies to \n",
        "\n",
        "$\\delta^{l}= \\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    (W^{l+1})^{T}\\delta^{l+1}_{1}⊙σ ^{'}(z_{1}^l)   & (W^{l+1})^{T}\\delta^{l+1}_{2}⊙σ ^{'}(z_{2}^l) & . & . &. &(W^{l+1})^{T}\\delta^{l+1}_{m}⊙σ ^{'}(z_{m}^l)   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix} \n",
        "$\n",
        "\n",
        "- *Looking at the above equation it is clear that $i^{th}$ column in $\\delta^l$ corresponds to $i^{th}$ data point in mini-batch*\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Analyzing Equation 4: $\\frac{∂C}{∂b^l} = \\delta^{l}$**\n",
        "\n",
        "- Since $\\delta^{l}=\n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    \\delta^{l}_{1}   & \\delta^{l}_{2} & . & . &. &\\delta^{l}_{m}   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}$ and each columns values corresponds to $i^{th}$ data instance(From above argument)\n",
        "\n",
        "To compute mini-batch gradient $\\frac{∂C}{∂b^l} = \\frac{\\sum_{i=1}^{m}\\frac{∂C}{∂b^l_i}}{m} = \\frac{\\sum_{i=1}^{m}\\delta^{l}_{i}}{m} \\tag{10}$\n",
        "\n",
        "\n",
        "*In order to compute the gradient update for the mini-batch data, we need to take the average of all columns vectores in $\\delta^{l}$ matrix*\n",
        "\n",
        "`np.sum(axis=0)`\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Analyzing Equation 5: $\\frac{∂C}{∂w^l} = \\delta^{l} (a^{l-1})^{T}$**\n",
        "\n",
        "\n",
        "- Since $\\delta^{l}=\n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    \\delta^{l}_{1}   & \\delta^{l}_{2} & . & . &. &\\delta^{l}_{m}   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}$ and $a^{l-1}=\n",
        "\\begin{bmatrix}\n",
        "    \\vert & \\vert &\\vert & \\vert &\\vert & \\vert \\\\\n",
        "    a^{l-1}_{1}   & a^{l-1}_{2} & . & . &. &a^{l-1}_{m}   \\\\\n",
        "\\vert & \\vert &\\vert & \\vert &\\vert & \\vert\n",
        "\\end{bmatrix}$\n",
        "\n",
        "\n",
        "\n",
        "- The product $\\delta^{l}(a^{l-1})^T$ becomes $\\delta^{l}_{1}(a^{l-1}_{1})^T+\\delta^{l}_{2}(a^{l-1}_{2})^T+. +. +.+ \\delta^{l}_{m}(a^{l-1}_{m})^T $\n",
        "\n",
        "Each quantity $\\delta^{l}_{i}(a^{l-1}_{i})^T$  in the summation represents a matrix for the $i^{th}$ instance. Since gradeint for the entire mini-batch involves summation for all the data instaces, the product $\\delta^{l}(a^{l-1})^T$ capures the summation information\n",
        "\n",
        "\n",
        "To compute mini-batch gradient $\\frac{∂C}{∂w^l} = \\frac{\\sum_{i=1}^{m}\\frac{∂C}{∂w^l_i}}{m} = \\frac{\\sum_{i=1}^{m}\\delta^{l}_{i}(a_i^{l-1})^T}{m} \\tag{10}$\n",
        "\n",
        "- From the Equation 10: Numerator of this expression is equal given by $\\delta^{l}(a^{l-1})^T$\n",
        "\n",
        "***In Summary, In order to copmute the gradient update of weights for the mini-batch data, we need to divide the matrix $\\delta^{l}(a^{l-1})^T$\n",
        "with mini-batch size (m)***\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "The conclusion of the above mathematical analysis are the following:\n",
        "\n",
        "- Assuming the data poinsts $x_i$ and $y_i$ are stacked along the column to make $X$ and $Y$\n",
        "\n",
        "- Using the numpy broadcasting operation during the fowards pass\n",
        "\n",
        "- We have to make change only in two places\n",
        "\n",
        "  - Computing the average of column vectors in $\\delta^{l}$ matrix\n",
        "  - Diving the matrix $\\delta^{l}(a^{l-1})^T$ with the batch-size $m$\n",
        "\n",
        "The result of this gives gradients with respect to weights and biases for the mini-batch data with out for loops\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rZT0cu3utrdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)  \n",
        "        self.sizes = sizes            \n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]] \n",
        "                                      \n",
        "                                      \n",
        "        self.weights = [np.random.randn(y, x) \n",
        "                                              \n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):        \n",
        "        for b, w in zip(self.biases, self.weights): \n",
        "            a = sigmoid(np.dot(w, a)+b) \n",
        "        return a\n",
        "\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
        "        gradient for the cost function C_x.  \"nabla_b\" and\n",
        "        \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
        "        to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        m = len(x)\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = (activations[-1] - y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta         # From the above mathematical anaysis, it is \n",
        "                                    # clear that we need to take the average along the column dimension\n",
        "        nabla_b[-1] = np.mean(nabla_b[-1], axis=1)  #( takeing average and axis=1 represents along the column dimension)\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "                                    # From the above mathematical analysis, it is clear that we need to divide the nabla_w with the batch size\n",
        "        nabla_w[-1] = nabla_w[-1] / m\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta   # From the above mathematical anaysis, it is \n",
        "                                    # clear that we need to take the average along the column dimension\n",
        "            nabla_b[-l] = np.mean(nabla_b[-l], axis=1) #( takeing average and axis=1 represents along the column dimension)\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "                                  # From the above mathematical analysis, it is clear that we need to divide the nabla_w with the batch size\n",
        "\n",
        "            nabla_w[-l] = nabla_w[-l] /m\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta):\n",
        "        \"\"\"Train the neural network using mini-batch stochastic\n",
        "        gradient descent.  The ``training_data`` is a list of tuples\n",
        "        ``(x, y)`` representing the training inputs and the desired\n",
        "        outputs.  The other non-optional parameters are\n",
        "        self-explanatory.  \"\"\"\n",
        "        n = len(training_data)\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Update the network's weights and biases by applying\n",
        "        gradient descent using backpropagation to a single mini batch.\n",
        "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
        "        is the learning rate.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "#        for x, y in mini_batch:\n",
        "#            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "#            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "#            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        X,Y = mini_batch                       # Storing the entire batch of data into X and Y matrices.\n",
        "        nabla_b , nabla_w = self.backprop(X,Y) # backprop takes X, Y and return the average of gradients \n",
        "                                               # of all the images in a batch without for loop\n",
        "\n",
        "        \n",
        "#        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "#                        for w, nw in zip(self.weights, nabla_w)]\n",
        "#        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "#                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "        self.weights = [w-(eta)*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)] \n",
        "        self.biases = [b-(eta)*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "        # I removed the denominator len(mini_batch) since we are getting averages from the backprop function\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "W-511l0GtaU4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Network([784,30,10])"
      ],
      "metadata": {
        "id": "SksBZun82R4f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.feedforward(np.random.randn(784,1))"
      ],
      "metadata": {
        "id": "QcaUSIe72UwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971cd0be-3f07-40e6-fc61-bba730e37aa9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.97552067e-03],\n",
              "       [7.22695924e-04],\n",
              "       [9.96692507e-01],\n",
              "       [8.26339636e-01],\n",
              "       [1.10037233e-02],\n",
              "       [9.96979140e-01],\n",
              "       [2.68942117e-01],\n",
              "       [1.67113032e-04],\n",
              "       [6.27497188e-04],\n",
              "       [9.05265013e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= np.random.randn(784,1000)\n",
        "Y= np.random.randn(10,1000)"
      ],
      "metadata": {
        "id": "XtjYNXYK3Zdc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_b2,nabla_w2 = net.backprop(X,Y)"
      ],
      "metadata": {
        "id": "jpARU-y1jXbM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_w2[-1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djepFCMTjS9e",
        "outputId": "82b73c74-9479-47b8-e0cb-421323a072af"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_b2[-1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzg8l4cS3qGi",
        "outputId": "5cb6ea24-0c30-4870-cd40-41e7931d23c4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1= X[:,0]\n",
        "x1=x1.reshape(784,1)\n",
        "print(x1.shape)\n",
        "y1= Y[:,0]\n",
        "y1=y1.reshape(10,1)\n",
        "print(y1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGZ5YkaFj_vQ",
        "outputId": "e08729f9-3f96-43c3-f248-8ab606b7bccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 1)\n",
            "(10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ4upefdQ6-W",
        "outputId": "b672afe0-a7c7-408a-ce08-4bbc4dd0ed38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_b,nabla_w = net.backprop(x1,y1)"
      ],
      "metadata": {
        "id": "aVCtBQYwj0mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_b[-2]"
      ],
      "metadata": {
        "id": "8vR9eybwRz_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_w = np.zeros(shape=(10,30))\n",
        "for i in range(1000):\n",
        "    \n",
        "    x1= X[:,i]\n",
        "    x1=x1.reshape(784,1)\n",
        "    y1= Y[:,i]\n",
        "    y1=y1.reshape(10,1)\n",
        "    a,b = net.backprop(x1,y1)\n",
        "    nabla_w = nabla_w + b[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "SsxzRUIPRKyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_w "
      ],
      "metadata": {
        "id": "aAwXps2KSvP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nabla_w2[-1]"
      ],
      "metadata": {
        "id": "696LUhONSwVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! jupyter nbconvert --to html '/content/drive/MyDrive/Colab Notebooks/Assignment_1.ipynb'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJqgjBEcS6zW",
        "outputId": "fbccab4f-2c9a-4fa1-d909-ba4bc0f34e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/Assignment_1.ipynb to html\n",
            "[NbConvertApp] Writing 338412 bytes to /content/drive/MyDrive/Colab Notebooks/Assignment_1.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0-2AGa9YNPQ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}